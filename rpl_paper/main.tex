\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{fontspec}
\usepackage{unicode-math}

\title{Resonant Phase-Locking: Stabilizing Long-Horizon Reasoning and Continual Adaptation via Phase-Synchronized Feature Routing}
\author{Payton Ison}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We introduce \emph{Resonant Phase-Locking} (RPL), a simple and general inductive bias for stabilizing long-horizon reasoning and continual adaptation in transformer models. RPL augments the residual stream with a lightweight oscillatory state---a phase $\theta$ with learned intrinsic frequency $\omega$---and gates attention and MLP updates by phase alignment. Inspired by the Kuramoto model of coupled oscillators, the model learns to synchronize phases along computational trajectories that should behave coherently over time while desynchronizing phases for independent subroutines. We implement RPL as (i) phase-modulated attention scores that reward attending to phase-aligned tokens, (ii) phase-aware MLP routing that gates neurons by their preferred rhythm, and (iii) regularizers that encourage smooth, task-dependent phase fields over long contexts. On synthetic and programmatic benchmarks, RPL improves (1) long-context generalization, (2) robustness to distractors, and (3) continual learning with reduced catastrophic forgetting under sequential tasks. Ablations show that the gains primarily arise from phase-gated routing rather than parameter count. We release reference code and a minimal implementation.
\end{abstract}

\section{Introduction}

Transformers excel at pattern completion but remain surprisingly brittle when asked to maintain coherent behavior over long horizons or across streams of changing tasks. Models trained on long sequences often fall back to local heuristics, lose track of nested structure, or overwrite earlier competencies when new tasks are introduced. Existing fixes---positional extrapolation tricks, auxiliary memory modules, or rehearsal for continual learning---tend to treat symptoms rather than give the model an internal notion of \emph{where it is} within a computation.

We propose \emph{Resonant Phase-Locking} (RPL): a minimal, differentiable mechanism that lets a model develop and maintain stable ``rhythms'' in its computation. Many reasoning traces are quasi-periodic: indentation cycles, list structures, planning loops, code blocks, dialogue turns, or recurrent subroutines in tool-using agents. If the model can represent its position within these cycles and align attention and routing with phase, it can more robustly sustain behaviors against noise, length, and interference.

RPL treats features as weakly coupled oscillators. Features that should move together in time (e.g., tokens in the same syntactic span or steps of the same plan) learn to phase-lock, while independent skills occupy disjoint phase clusters. The core idea is simple: attach a phase variable to each token state, let it evolve under a Kuramoto-like update informed by attention, and use phase similarity to gate communication and routing.

\paragraph{Contributions.} This work:
\begin{itemize}
    \item Introduces a phase-augmented transformer architecture in which each token carries a learned oscillatory state whose dynamics are coupled through attention.
    \item Derives a synchronization perspective on long-horizon stability, relating RPL to coupled-oscillator theory via the Kuramoto order parameter and effective coupling.
    \item Empirically demonstrates improved long-context generalization, distraction robustness, and reduced catastrophic forgetting on synthetic, programmatic, and sequential task streams.
\end{itemize}

\section{Method: Resonant Phase-Locking}

\subsection{Phase-augmented state}

Consider a standard transformer layer with residual states $x_t \in \mathbb{R}^d$ for tokens $t = 1,\dots,T$. RPL augments each state with a scalar phase $\theta_t \in \mathbb{R}$ and uses its 2D embedding
\[
p_t = (\cos \theta_t, \sin \theta_t)
\]
as a compact representation of ``where in the local rhythm'' the token currently is.

A small head predicts an intrinsic frequency $\omega_t$ from the residual,
\[
\omega_t = W_\omega x_t + b_\omega,
\]
and phases evolve via a learned Kuramoto-like update:
\begin{equation}
\theta_{t}^{\text{next}} = \theta_t + \omega_t + \kappa \cdot \mathrm{Agg}_{j \in \mathcal{N}(t)} w_{tj} \sin(\theta_j - \theta_t),
\label{eq:phase_update}
\end{equation}
where $\kappa$ is a learnable coupling scalar, $w_{tj}$ are neighborhood weights, and $\mathrm{Agg}$ is an aggregation operator. In practice, we use the transformer’s self-attention to define the neighborhood: $\mathcal{N}(t)$ is the set of tokens attended to by $t$, and $w_{tj}$ are the corresponding attention weights.

Thus, attention not only routes information in feature space but also couples phases along active computational subgraphs.

\subsection{Phase-gated attention}

Given queries $q_i$, keys $k_j$, and token phases $\theta_i,\theta_j$, we modulate attention logits by phase alignment. Let the base logits be
\[
\ell_{ij}^{\text{base}} = \frac{q_i^\top k_j}{\sqrt{d}}.
\]
RPL adds a phase term,
\begin{equation}
\ell_{ij} = \ell_{ij}^{\text{base}} + \lambda \cos(\theta_i - \theta_j),
\label{eq:phase_attn_logits}
\end{equation}
where $\lambda$ is a learned scalar or per-head parameter. The resulting attention weights are
\[
a_{ij} = \operatorname{softmax}_j(\ell_{ij}).
\]
This biases the model to attend preferentially to tokens that share a compatible rhythm with the current token for the active feature subspace.

We also gate value mixing by a simple phase-dependent gate applied channelwise. Given values $v_j$ and phase difference $\Delta\theta_{ij} = \theta_i - \theta_j$, we define
\begin{equation}
g(\Delta\theta_{ij}) = \operatorname{ReLU}\big(\cos(\Delta\theta_{ij})\big),
\label{eq:phase_gate}
\end{equation}
and compute the layer output as
\begin{equation}
y_i = \sum_j a_{ij} \, g(\Delta\theta_{ij}) \, v_j.
\end{equation}
When phases are aligned ($\Delta\theta \approx 0$), the gate is near 1; when phases are opposed, the gate is suppressed.

\subsection{Phase-aware MLP routing}

Let $h_i$ denote the post-attention residual at token $i$. RPL introduces a phase-conditioned gate $\rho(\theta_i) \in [0,1]^d$ that modulates which MLP channels are open:
\[
\rho(\theta_i) = \sigma_\rho\!\big(W_\rho \, [\cos \theta_i, \sin \theta_i]^\top + b_\rho\big),
\]
where $\sigma_\rho$ is a smooth nonlinearity (e.g., sigmoid) and $W_\rho$ is tiny (dimension $d \times 2$). The MLP pre-activation becomes
\begin{equation}
z_i = W_2 \, \sigma\!\big( W_1 \, (h_i \odot \rho(\theta_i)) \big),
\label{eq:mlp_routing}
\end{equation}
where $\odot$ denotes elementwise multiplication. Neurons specialize to specific phase bands; when the local phase matches their preference, they dominate the computation.

\subsection{Regularizers}

RPL adds two lightweight regularizers on top of the task loss $\mathcal{L}_{\mathrm{task}}$.

\paragraph{Coherence.} We encourage local synchrony where attention is high. Let $a_{ij}$ denote attention weights. A coherence term is
\begin{equation}
\mathcal{L}_{\mathrm{coh}} = - \mathbb{E}\Big[ \sum_{i,j} a_{ij} \cos(\theta_i - \theta_j) \Big],
\label{eq:coherence}
\end{equation}
which rewards phase-aligned high-attention pairs.

\paragraph{Separation.} To prevent trivial global locking where all phases collapse, we add a contrastive term that pushes unrelated regions apart. For pairs $(i,j)$ with low attention (e.g., $a_{ij}$ below a threshold), we define
\begin{equation}
\mathcal{L}_{\mathrm{sep}} = \mathbb{E}\Big[ \max\big(0, \cos(\theta_i - \theta_j) - \tau\big) \Big],
\label{eq:separation}
\end{equation}
where $\tau \in (0,1)$ is a margin. If low-attention pairs become too aligned in phase, the loss increases.

\paragraph{Total objective.} The full training loss is
\begin{equation}
\mathcal{L} = \mathcal{L}_{\mathrm{task}} + \alpha \mathcal{L}_{\mathrm{coh}} + \beta \mathcal{L}_{\mathrm{sep}},
\end{equation}
with tunable coefficients $\alpha, \beta$.

\section{Synchronization Perspective and Stability}

Let $\Theta = \{\theta_1,\dots,\theta_N\}$ be the phases of tokens/features in a computation subgraph. Define the Kuramoto order parameter
\begin{equation}
r e^{i\psi} = \frac{1}{N} \sum_{n=1}^N e^{i \theta_n},
\end{equation}
where $r \in [0,1]$ measures the degree of synchronization and $\psi$ is the mean phase.

For a broad class of coupling functions, mean-field analysis of the Kuramoto model yields a critical coupling $\kappa_c$ above which partial synchronization ($r > 0$) emerges. Below $\kappa_c$, phases drift independently and $r \approx 0$; above it, a macroscopic fraction of oscillators lock into a coherent cluster.

In RPL, attention weights bias the effective neighborhood $\mathcal{N}(t)$ and induce a task-dependent effective coupling $\kappa_{\mathrm{eff}}$. Subgraphs corresponding to persistent subroutines (e.g., tracking nesting depth, maintaining a loop invariant, or following a pointer chain) experience stronger coupling, while noisy or irrelevant tokens exert weak influence.

\paragraph{Stability intuition.} When a subroutine must persist, the model can increase $\kappa_{\mathrm{eff}}$ along that subgraph by directing attention to phase-compatible tokens, thereby increasing $r$ and reducing drift of the shared phase field. Distractors that do not phase-align see reduced effective influence through the $\cos(\Delta\theta)$ gate. This yields a soft, differentiable notion of working memory: information is stabilized not by explicit external buffers, but by locking the internal phases of relevant features into a coherent rhythm.

\section{Experiments}

\paragraph{Benchmarks.} We evaluate RPL on:
\begin{enumerate}
    \item \textbf{Long-horizon synthetic tasks.} Dyck-$k$ languages (nesting depth), addition/multiplication with carry over long contexts, and pointer following with distractors.
    \item \textbf{Programmatic tasks.} Indentation prediction, bracket closing under adversarial comments, and variable-type tracking across long-distance dependencies.
    \item \textbf{Continual learning.} A stream of disjoint tasks (e.g., 10 tasks in sequence), trained without rehearsal.
\end{enumerate}

\paragraph{Metrics.} We track:
\begin{itemize}
    \item Long-context generalization: accuracy as a function of sequence length at test time.
    \item Distraction robustness: performance in the presence of irrelevant or adversarial tokens.
    \item Forgetting: performance drop from a task's peak accuracy to its accuracy after subsequent tasks.
    \item Synchronization dynamics: trajectories of the order parameter $r$ over the course of a forward pass.
\end{itemize}

\paragraph{Ablations.} To isolate where gains come from, we consider:
\begin{itemize}
    \item Removing attention gating (setting $\lambda = 0$ in Eq.~\eqref{eq:phase_attn_logits}).
    \item Removing MLP routing (replacing $\rho(\theta)$ with an all-ones gate).
    \item Freezing the phase dynamics (no coupling, $\kappa = 0$, or fixed random phases).
    \item Parameter-matched baselines where we add extra heads or channels instead of phase variables.
\end{itemize}

\paragraph{Results (summary).} Across settings, RPL:
\begin{itemize}
    \item Improves length generalization on Dyck-$k$ and arithmetic tasks, with accuracy degrading more gracefully as length grows.
    \item Reduces forgetting in sequential task streams, maintaining higher performance on earlier tasks without rehearsal.
    \item Exhibits interpretable phase fields, where persistent subroutines correspond to stable, localized phase clusters with elevated $r$.
\end{itemize}
Representative curves for forgetting and synchronization are shown in Fig.~\ref{fig:forget} and Fig.~\ref{fig:phase}. We release reference code for reproducibility.

\begin{figure}[t]
\centering
\includegraphics[width=.8\linewidth]{figs/phase_lock_K0_0.png}
\caption{Illustrative phase-locking behavior on a synthetic long-context task. As coupling strengthens, the order parameter $r$ increases along relevant subroutines while remaining low elsewhere.}
\label{fig:phase}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=.8\linewidth]{figs/forgetting_curve.png}
\caption{Forgetting curves on a sequential task stream. RPL (solid) retains earlier tasks better than a parameter-matched baseline (dashed).}
\label{fig:forget}
\end{figure}

\section{Discussion}

RPL adds a small number of parameters and operations but introduces a qualitatively new inductive bias: computations are structured as phase-synchronized rhythms rather than purely as static message passing. This offers several benefits:
\begin{itemize}
    \item \textbf{Stability.} Persistent subroutines can be maintained via phase-locking, providing a differentiable analogue of working memory.
    \item \textbf{Selective routing.} Phase gates filter out off-rhythm interference, improving robustness to distractors and noisy context.
    \item \textbf{Interpretability.} Phase fields and order-parameter trajectories provide a new window into internal computation, revealing when and where the model recruits synchronized substructures.
\end{itemize}

Limitations include additional computational cost from phase updates, potential sensitivity to hyperparameters $(\lambda,\kappa,\alpha,\beta)$, and the possibility of degeneracies where the model finds trivial phase configurations. Future work includes applying RPL to real-world agents, multi-modal sequences, and memory-augmented transformers, as well as exploring learned frequency spectra and hierarchical phase structures.

\section*{Appendix: Pseudocode}

\begin{verbatim}
# Pseudocode for an RPL-augmented attention block (PyTorch-like)

def rpl_attention(x, theta, Wq, Wk, Wv, Wo, omega, lambda_phase, kappa):
    """
    x:      [T, d] token representations
    theta:  [T]    phases for each token
    Wq,Wk,Wv,Wo: projection matrices
    omega:  small network mapping x -> intrinsic frequency
    """
    # standard projections
    q = x @ Wq
    k = x @ Wk
    v = x @ Wv

    # base attention logits
    d = q.shape[-1]
    base_logits = (q @ k.T) / math.sqrt(d)

    # phase alignment term
    dtheta = theta[:, None] - theta[None, :]
    phase_term = lambda_phase * torch.cos(dtheta)
    logits = base_logits + phase_term

    # attention weights
    attn = torch.softmax(logits, dim=-1)

    # phase-coupled update step (Kuramoto-like)
    coupling = (attn * torch.sin(dtheta)).sum(dim=-1)
    theta_next = theta + omega(x) + kappa * coupling

    # value mixing, gated by ReLU(cos Δθ)
    gate = F.relu(torch.cos(dtheta))
    y = (attn[..., None] * v[None, ...] * gate[..., None]).sum(dim=1)

    return y @ Wo, theta_next
\end{verbatim}

\end{document}
