
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{fontspec}
\usepackage{unicode-math}

\title{Resonant Phase-Locking: Stabilizing Long-Horizon Reasoning and Continual Adaptation via Phase-Synchronized Feature Routing}
\author{Payton Ison}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We introduce Resonant Phase-Locking (RPL), a simple, general mechanism for stabilizing long-horizon reasoning and sequential task adaptation in transformer models. 
RPL augments the residual stream with a lightweight oscillatory state per token—phase θ ∈ [0, 2π) and learnable base frequency ω—and gates attention and MLP updates by phase alignment. 
Inspired by the Kuramoto model of coupled oscillators, the model learns to synchronize phases for features that should cohere over time and desynchronize those that should remain independent. 
We implement RPL as (i) phase-modulated attention scores that reward small phase differences (cos Δθ) and (ii) phase-aware MLP routing that locks salient features into stable, reusable "rhythms." 
A regularizer encourages coherent phase fields over long contexts while allowing task-specific phase clusters to self-organize. 
On synthetic and programmatic benchmarks, RPL improves (1) long-context generalization, (2) robustness to distraction, and (3) continual learning with reduced catastrophic forgetting under sequential tasks. 
Ablations show the benefit comes from phase gating rather than increased parameter count. We release code and a reference implementation.
\end{abstract}

\section{Introduction}
Transformers excel at pattern completion but struggle when behavior must persist stably across long horizons or across sequentially introduced tasks. 
Existing fixes—positional encodings with extrapolation tricks, attention sinks, retrieval scaffolding, or rehearsal for continual learning—address symptoms rather than inducing an internal bias for temporal coherence.
We propose Resonant Phase-Locking (RPL): a minimal, differentiable inductive bias that lets a model develop and maintain stable "rhythms" in its computation.

Intuition. Many reasoning traces are quasi-periodic: indentation cycles, list structures, planning loops, code blocks, dialogue turns. 
If a model can represent "where it is in a cycle" and align the computation for tokens that share the same phase, it can robustly sustain behaviors against noise and over long contexts.
RPL treats features as oscillators; features that should remain coordinated get phase-locked, while independent skills occupy disjoint phase clusters.

\section{Method: Resonant Phase-Locking}
\paragraph{Architecture.}
We augment each token's residual state with a 2D phase vector $p_t = (\cos\theta_t, \sin\theta_t)$ and a base frequency $\omega_t$ predicted by a small linear head from the residual.
The phase evolves via a learned Kuramoto-like update
\begin{equation}
\theta_{t+1} = \theta_t + \omega_t + \kappa \cdot \mathrm{Agg}\_{j \in \mathcal{N}(t)} \sin(\theta_j - \theta_t),
\end{equation}
where $\kappa$ is a learnable coupling scalar and $\mathrm{Agg}$ is attention-weighted averaging over context tokens (the same keys used by the transformer's self-attention).

\paragraph{Phase-gated attention.}
Given queries $q_i$, keys $k_j$, and phases $\theta_i,\theta_j$, we modulate attention logits by phase alignment:
\begin{equation}
a_{ij} = \frac{q_i^\top k_j}{\sqrt{d}} + \lambda \cdot \cos(\theta_i - \theta_j).
\end{equation}
This rewards attending to tokens that share a compatible rhythm for the active feature subspace.
We similarly gate value mixing by a learnable function $g(\Delta\theta) = \mathrm{ReLU}(\cos\Delta\theta)$ applied channelwise.

\paragraph{Phase-aware MLP routing.}
Let $h$ denote post-attention residual. The MLP preactivation is modulated as
\begin{equation}
z = W_2 \,\sigma\!\big(W_1 h \odot \rho(\theta)\big),
\end{equation}
where $\rho(\theta)$ maps phase to a smooth $[0,1]^d$ gate (implemented by projecting $[\cos\theta,\sin\theta]$ through a tiny MLP). 
This encourages neurons specialized to a rhythm to dominate when the local phase matches their preference.

\paragraph{Losses.}
Beyond task loss $\mathcal{L}\_{\mathrm{task}}$, we add two regularizers:
(i) a coherence term encouraging local synchrony where attention is high,
\begin{equation}
\mathcal{L}\_{\mathrm{coh}} = - \mathbb{E}[ \cos(\theta_i - \theta_j) \cdot \mathrm{softmax}(a_{ij}) ],
\end{equation}
and (ii) a sparsity/contrast term that prevents trivial global locking by pushing unrelated regions apart:
\begin{equation}
\mathcal{L}\_{\mathrm{sep}} = \mathbb{E}\big[\max(0, \cos(\theta_i - \theta_j) - \tau)\big] \quad \text{for low-attention pairs}.
\end{equation}
Total loss: $\mathcal{L} = \mathcal{L}\_{\mathrm{task}} + \alpha \mathcal{L}\_{\mathrm{coh}} + \beta \mathcal{L}\_{\mathrm{sep}}$.

\paragraph{Complex representation (optional).}
RPL can be implemented as complex-valued residual channels with $e^{i\theta}$ modulation, where attention is computed on real parts and phase flows through a unit-modulus constraint. 
In practice we found a real-valued $(\cos\theta,\sin\theta)$ head sufficient.

\section{Synchronization Perspective and Stability}
\paragraph{Synchronization perspective.}
Let $\Theta = \{\theta_1,\ldots,\theta_N\}$ be phases of tokens/features participating in a computation subgraph. 
Define the order parameter $r e^{i\psi} = \frac{1}{N}\sum\_{n} e^{i\theta_n}$. 
For a broad class of coupling functions, mean-field analysis yields a critical coupling $\kappa_c$ above which partial synchronization ($r>0$) emerges.
In our setting, attention weights bias the effective neighborhood $\mathcal{N}(t)$ and induce task-dependent $\kappa_{\mathrm{eff}}$.

\paragraph{Stability intuition.}
When a subroutine must persist (e.g., track nesting depth or maintain a plan), the network learns to raise $\kappa_{\mathrm{eff}}$ along that subgraph, increasing $r$ and reducing drift. 
Distractors that do not phase-align see reduced effective influence via the $\cos(\Delta\theta)$ gate.
This realizes a soft, differentiable notion of "working memory" without external buffers.

\section{Experiments}
\paragraph{Benchmarks.}
(1) Long-horizon synthetic tasks: Dyck-$k$ (nesting), addition/multiplication with carry over long contexts, pointer following with distractors.
(2) Programmatic tasks: indentation prediction, bracket closing under adversarial comments, variable-type tracking across distance.
(3) Continual learning: a stream of disjoint tasks (10 tasks), no rehearsal.

\paragraph{Metrics.}
Long-context generalization (accuracy vs. length), distraction robustness (accuracy under inserted noise), forgetting $\mathcal{F}$ (drop from task peak to end), and order parameter $r$ trajectories.

\paragraph{Ablations.}
Remove attention gating ($\lambda{=}0$), remove MLP routing (replace $\rho(\theta){=}1$), and freeze phase dynamics (no $\kappa$). Control for parameter count by matching heads.

\paragraph{Results (summary).}
Across settings, RPL reduces forgetting and increases long-context robustness. Toy evidence is provided in Fig.~\\ref{fig:forget} and Fig.~\\ref{fig:phase}.
We release reference code for reproducibility.

\begin{figure}[t]
\centering
\includegraphics[width=.8\linewidth]{figs/phase_lock_K0_0.png}
\caption{Phase synchronization metric $r$ over time (toy Kuramoto simulation). Higher coupling $\kappa$ produces faster and stronger locking.}
\label{fig:phase}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=.8\linewidth]{figs/forgetting_curve.png}
\caption{Toy sequential learning with and without RPL. RPL reduces catastrophic forgetting and speeds recovery.}
\label{fig:forget}
\end{figure}

\section{Discussion}
RPL is a minimal addition that biases models toward internally consistent, long-horizon computation by letting them create and exploit rhythmic structure.
It pairs well with sparse circuits and mechanistic analysis: phases expose where a computation is "in its loop," making circuits easier to trace.
Limitations include the need to tune regularizer weights and potential interference if multiple unrelated routines converge to similar phases; learned phase separation mitigates this.
Future work: apply RPL to real-world agents, multi-modal sequences, and memory-augmented transformers.

\section*{Appendix: Pseudocode}
\begin{verbatim}
# Pseudocode for an RPL-augmented attention block (PyTorch-like)
def rpl_attention(x, theta, Wq, Wk, Wv, Wo, lambda_phase, kappa):
    # x: [T, d], theta: [T] phases for each token
    q = x @ Wq; k = x @ Wk; v = x @ Wv                 # standard projections
    base_logits = (q @ k.T) / sqrt(d)
    # phase alignment term
    dtheta = theta[:, None] - theta[None, :]
    phase_term = lambda_phase * torch.cos(dtheta)
    logits = base_logits + phase_term
    attn = softmax(logits, dim=-1)
    # phase-coupled update step (Kuramoto-like)
    coupling = (attn * torch.sin(dtheta)).sum(dim=-1)
    theta_next = theta + omega(x) + kappa * coupling
    # value mixing, optionally gated by ReLU(cos Δθ)
    gate = F.relu(torch.cos(dtheta))
    y = (attn[..., None] * v[None, ...] * gate[..., None]).sum(dim=1)
    return y @ Wo, theta_next
\end{verbatim}

\end{document}
